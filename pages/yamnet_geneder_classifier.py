# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ek1x6If_wWKLJgcijw-KR6SI6ca3Vke1
"""

# ============================
# Cell 1: Installs & Imports
# ============================
!pip install --upgrade pip setuptools >/dev/null
!pip install kagglehub librosa soundfile tensorflow matplotlib scikit-learn tqdm >/dev/null
!pip install tensorflow_hub >/dev/null

import os, shutil, random
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import librosa, soundfile as sf
import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras import layers, models, callbacks
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
from google.colab import files

print("‚úÖ Packages installed and imported.")


# ============================
# Cell 2: Download dataset
# ============================
import kagglehub

print("‚¨áÔ∏è Downloading Gender Voice Dataset...")
path = kagglehub.dataset_download("murtadhanajim/gender-recognition-by-voiceoriginal")
print("‚úÖ Dataset downloaded to:", path)

DATA_ROOT = Path(path) / "data"
FEMALE_DIR = DATA_ROOT / "female"
MALE_DIR = DATA_ROOT / "male"

print(f"Female samples: {len(list(FEMALE_DIR.glob('*.wav')))}")
print(f"Male samples: {len(list(MALE_DIR.glob('*.wav')))}")


# ============================
# Cell 3: Audio loading helper
# ============================
def safe_load_audio(path, sr=16000, duration=3.0):
    """Load audio, pad/trim to fixed duration."""
    try:
        audio, orig_sr = librosa.load(str(path), sr=sr, mono=True)
    except Exception as e:
        try:
            audio, orig_sr = sf.read(str(path))
            if audio.ndim > 1:
                audio = np.mean(audio, axis=1)
            if orig_sr != sr:
                audio = librosa.resample(audio, orig_sr, sr)
        except Exception as e2:
            print(f"‚ö†Ô∏è Error loading {path}: {e2}")
            return None
    target_len = int(sr * duration)
    if len(audio) < target_len:
        audio = np.pad(audio, (0, target_len - len(audio)))
    else:
        audio = audio[:target_len]
    return audio


# ============================
# Cell 4: YAMNet feature extraction
# ============================
print("üîç Loading YAMNet model...")
yamnet_model = hub.load("https://tfhub.dev/google/yamnet/1")

def extract_embedding(audio):
    """Extract averaged YAMNet embedding for one clip."""
    scores, embeddings, spectrogram = yamnet_model(audio)
    return tf.reduce_mean(embeddings, axis=0).numpy()

def gather_audio_files(folder):
    exts = ("*.wav", "*.mp3", "*.flac")
    files = []
    for e in exts:
        files.extend(folder.glob("**/" + e))
    return files


female_files = gather_audio_files(FEMALE_DIR)
male_files = gather_audio_files(MALE_DIR)

print(f"Found {len(female_files)} female, {len(male_files)} male files.")


# ============================
# Cell 5: Build dataset
# ============================
X_list, y_list = [], []

print("üéº Extracting embeddings... (this may take a few minutes)")

for f in tqdm(female_files, desc="Female"):
    audio = safe_load_audio(f)
    if audio is None: continue
    emb = extract_embedding(tf.convert_to_tensor(audio, dtype=tf.float32))
    X_list.append(emb)
    y_list.append(1)  # Female = 1

for f in tqdm(male_files, desc="Male"):
    audio = safe_load_audio(f)
    if audio is None: continue
    emb = extract_embedding(tf.convert_to_tensor(audio, dtype=tf.float32))
    X_list.append(emb)
    y_list.append(0)  # Male = 0

X = np.array(X_list)
y = np.array(y_list)
print("‚úÖ Final dataset:", X.shape, y.shape)


# ============================
# Cell 6: Split & Class balance
# ============================
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

print("Train/Val/Test sizes:", X_train.shape[0], X_val.shape[0], X_test.shape[0])

class_weights = compute_class_weight("balanced", classes=np.unique(y), y=y)
class_weight_dict = dict(zip(np.unique(y), class_weights))
print("‚öñÔ∏è Class weights:", class_weight_dict)


# ============================
# Cell 7: Classifier model
# ============================
print("üß† Building classifier...")
input_shape = X_train.shape[1:]

classifier = models.Sequential([
    layers.Input(shape=input_shape),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.3),
    layers.Dense(64, activation="relu"),
    layers.Dropout(0.3),
    layers.Dense(2, activation="softmax")
])

classifier.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
classifier.summary()


# ============================
# Cell 8: Training
# ============================
EPOCHS = 20
BATCH = 32
es = callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)

history = classifier.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=EPOCHS, batch_size=BATCH,
    callbacks=[es],
    class_weight=class_weight_dict
)

MODEL_PATH = "/content/yamnet_gender_classifier.h5"
classifier.save(MODEL_PATH)
print(f"üíæ Model saved to: {MODEL_PATH}")


# ============================
# Cell 9: Evaluation
# ============================
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='train_acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.title('Accuracy'); plt.xlabel('Epoch'); plt.ylabel('Acc'); plt.legend()
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.title('Loss'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()
plt.show()

test_loss, test_acc = classifier.evaluate(X_test, y_test, verbose=0)
print(f"\nüéØ Final Test Accuracy: {test_acc*100:.2f}% | Test Loss: {test_loss:.4f}")

y_pred_probs = classifier.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

labels = ["Male", "Female"]
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=labels))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))


# ============================
# Cell 10: Test with uploaded file
# ============================
classes = ["Male", "Female"]

def classify_voice_file(file_path, sr=16000):
    audio = safe_load_audio(file_path, sr=sr, duration=3.0)
    if audio is None:
        return None
    emb = extract_embedding(tf.convert_to_tensor(audio, dtype=tf.float32))[np.newaxis, :]
    probs = classifier.predict(emb, verbose=0)[0]
    return {
        "Prediction": classes[np.argmax(probs)],
        "Male %": round(probs[0]*100, 1),
        "Female %": round(probs[1]*100, 1)
    }

while True:
    print("\nüé§ Upload a voice file (wav/mp3/flac) or type STOP to exit.")
    uploaded = files.upload()
    if not uploaded:
        break
    for fname in uploaded.keys():
        print(f"\nüîä Processing: {fname}")
        result = classify_voice_file(fname)
        print(result)
    cont = input("Test another file? (y/n): ").strip().lower()
    if cont != "y":
        print("‚úÖ Testing finished.")
        break